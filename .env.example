# ═══════════════════════════════════════════════════════════════════════
# DocGemma Configuration
# ═══════════════════════════════════════════════════════════════════════
# Copy this file to .env and fill in the required values:
#   cp .env.example .env
#
# Then choose a profile:
#   docker compose --profile remote up    # Remote vLLM endpoint
#   docker compose --profile gpu up       # Local GPU with vLLM
# ═══════════════════════════════════════════════════════════════════════

# ── Profile: remote ──────────────────────────────────────────────────
# Required when using --profile remote
DOCGEMMA_ENDPOINT=https://your-vllm-endpoint.example.com
DOCGEMMA_API_KEY=your-api-key-here

# ── Profile: gpu ─────────────────────────────────────────────────────
# Required when using --profile gpu (needs MedGemma access on HuggingFace)
HF_TOKEN=hf_your_huggingface_token_here

# ── Shared (optional) ───────────────────────────────────────────────
# DOCGEMMA_MODEL=google/medgemma-27b-it
# DOCGEMMA_PORT=8080
# VLLM_MAX_MODEL_LEN=8192
# VLLM_GPU_UTIL=0.90
