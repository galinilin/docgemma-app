{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DocGemma on Google Colab\n",
        "\n",
        "Agentic medical AI assistant powered by MedGemma, with autonomous tool calling for clinical decision support.\n",
        "\n",
        "This notebook deploys the full DocGemma stack (vLLM + backend + frontend) and exposes it via a public URL.\n",
        "\n",
        "**Requirements:**\n",
        "- GPU runtime (Runtime > Change runtime type > T4 or A100)\n",
        "- [HuggingFace token](https://huggingface.co/settings/tokens) with access to [MedGemma](https://huggingface.co/google/medgemma-27b-it)\n",
        "\n",
        "| GPU | VRAM | Model |\n",
        "|-----|------|-------|\n",
        "| T4 | 16 GB | MedGemma 1.5 4B |\n",
        "| A100 (40 GB) | 40 GB | MedGemma 1.5 4B |\n",
        "| A100 (80 GB) | 80 GB | MedGemma 27B |\n",
        "\n",
        "**Repos:** [docgemma-app](https://github.com/galinilin/docgemma-app) | [docgemma-connect](https://github.com/galinilin/docgemma-connect) | [docgemma-frontend](https://github.com/galinilin/docgemma-frontend)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. GPU Check & Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import subprocess, os\n",
        "\n",
        "# Check GPU\n",
        "result = subprocess.run([\"nvidia-smi\", \"--query-gpu=name,memory.total\", \"--format=csv,noheader,nounits\"],\n",
        "                        capture_output=True, text=True)\n",
        "if result.returncode != 0:\n",
        "    raise RuntimeError(\"No GPU detected. Go to Runtime > Change runtime type > select T4 or A100.\")\n",
        "\n",
        "gpu_name, gpu_mem = result.stdout.strip().split(\", \")\n",
        "gpu_mem = int(gpu_mem)\n",
        "print(f\"GPU: {gpu_name} ({gpu_mem} MB VRAM)\")\n",
        "\n",
        "# Auto-select model based on VRAM\n",
        "if gpu_mem >= 48000:\n",
        "    MODEL = \"google/medgemma-27b-it\"\n",
        "else:\n",
        "    MODEL = \"google/medgemma-1.5-4b-it\"\n",
        "\n",
        "print(f\"Model: {MODEL}\")\n",
        "\n",
        "# Ports\n",
        "VLLM_PORT = 8000\n",
        "APP_PORT = 8080\n",
        "WORKDIR = \"/content/docgemma\"\n",
        "os.makedirs(WORKDIR, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. HuggingFace Token\n",
        "\n",
        "Enter your HuggingFace token. You need access to the [MedGemma model](https://huggingface.co/google/medgemma-27b-it) (click \"Request access\" on the model page)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import getpass\n",
        "\n",
        "# Try Colab secrets first, fall back to manual input\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
        "    print(\"Using HF_TOKEN from Colab secrets.\")\n",
        "except Exception:\n",
        "    HF_TOKEN = getpass.getpass(\"Enter your HuggingFace token: \")\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = HF_TOKEN\n",
        "print(f\"Token set ({HF_TOKEN[:8]}...)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Install Dependencies\n",
        "\n",
        "Installs Node.js, UV (Python package manager), vLLM, and cloudflared (for the public tunnel)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%bash\n",
        "set -e\n",
        "\n",
        "echo \"=== Installing Node.js ===\"\n",
        "if ! command -v node &>/dev/null || [ \"$(node --version | grep -oP '(?<=v)\\d+')\" -lt 18 ]; then\n",
        "    curl -fsSL https://deb.nodesource.com/setup_20.x | bash - > /dev/null 2>&1\n",
        "    apt-get install -y -qq nodejs > /dev/null 2>&1\n",
        "fi\n",
        "echo \"Node.js $(node --version)\"\n",
        "\n",
        "echo \"=== Installing UV ===\"\n",
        "if ! command -v uv &>/dev/null; then\n",
        "    curl -LsSf https://astral.sh/uv/install.sh | sh 2>/dev/null\n",
        "fi\n",
        "export PATH=\"$HOME/.local/bin:$PATH\"\n",
        "echo \"UV $(uv --version)\"\n",
        "\n",
        "echo \"=== Installing cloudflared ===\"\n",
        "if ! command -v cloudflared &>/dev/null; then\n",
        "    curl -fsSL https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -o /usr/local/bin/cloudflared\n",
        "    chmod +x /usr/local/bin/cloudflared\n",
        "fi\n",
        "echo \"cloudflared $(cloudflared --version 2>&1 | head -1)\"\n",
        "\n",
        "echo \"=== Done ===\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install vLLM + HuggingFace CLI (takes a few minutes)\n",
        "!pip install -q vllm huggingface_hub\n",
        "!huggingface-cli login --token $HF_TOKEN 2>/dev/null\n",
        "print(\"vLLM + HuggingFace ready\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Clone & Build"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%bash\n",
        "set -e\n",
        "export PATH=\"$HOME/.local/bin:$PATH\"\n",
        "WORKDIR=\"/content/docgemma\"\n",
        "\n",
        "echo \"=== Cloning repositories ===\"\n",
        "if [ ! -d \"$WORKDIR/docgemma-connect\" ]; then\n",
        "    git clone --depth 1 https://github.com/galinilin/docgemma-connect.git \"$WORKDIR/docgemma-connect\"\n",
        "else\n",
        "    echo \"docgemma-connect already cloned\"\n",
        "fi\n",
        "\n",
        "if [ ! -d \"$WORKDIR/docgemma-frontend\" ]; then\n",
        "    git clone --depth 1 https://github.com/galinilin/docgemma-frontend.git \"$WORKDIR/docgemma-frontend\"\n",
        "else\n",
        "    echo \"docgemma-frontend already cloned\"\n",
        "fi\n",
        "\n",
        "echo \"=== Installing backend dependencies ===\"\n",
        "cd \"$WORKDIR/docgemma-connect\"\n",
        "uv sync --frozen --no-dev\n",
        "\n",
        "echo \"=== Building frontend ===\"\n",
        "cd \"$WORKDIR/docgemma-frontend\"\n",
        "npm install --silent 2>/dev/null\n",
        "VITE_API_URL=/api npm run build\n",
        "\n",
        "echo \"=== Copying frontend into backend ===\"\n",
        "mkdir -p \"$WORKDIR/docgemma-connect/static\"\n",
        "cp -r \"$WORKDIR/docgemma-frontend/dist/\"* \"$WORKDIR/docgemma-connect/static/\"\n",
        "\n",
        "echo \"=== Build complete ===\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Start vLLM\n",
        "\n",
        "Starts the vLLM inference server in the background and waits for it to be ready. First run downloads model weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import subprocess, time, urllib.request\n",
        "\n",
        "# Kill any existing vLLM process\n",
        "subprocess.run([\"pkill\", \"-f\", \"vllm.entrypoints\"], capture_output=True)\n",
        "time.sleep(2)\n",
        "\n",
        "vllm_cmd = [\n",
        "    \"vllm\", \"serve\", MODEL,\n",
        "    \"--max-model-len\", \"8192\",\n",
        "    \"--gpu-memory-utilization\", \"0.90\",\n",
        "    \"--host\", \"0.0.0.0\",\n",
        "    \"--port\", str(VLLM_PORT),\n",
        "]\n",
        "\n",
        "vllm_log = open(\"/content/vllm.log\", \"w\")\n",
        "vllm_proc = subprocess.Popen(vllm_cmd, stdout=vllm_log, stderr=subprocess.STDOUT)\n",
        "print(f\"vLLM starting (PID: {vllm_proc.pid})...\")\n",
        "print(f\"Model: {MODEL}\")\n",
        "print(\"Waiting for model to load (check /content/vllm.log for progress)...\")\n",
        "\n",
        "# Wait for health endpoint\n",
        "for i in range(360):  # 30 minutes max\n",
        "    try:\n",
        "        urllib.request.urlopen(f\"http://localhost:{VLLM_PORT}/health\", timeout=2)\n",
        "        print(f\"\\nvLLM is ready! (took ~{i * 5}s)\")\n",
        "        break\n",
        "    except Exception:\n",
        "        if vllm_proc.poll() is not None:\n",
        "            print(\"\\nvLLM process died. Last 30 lines of log:\")\n",
        "            !tail -30 /content/vllm.log\n",
        "            raise RuntimeError(\"vLLM failed to start.\")\n",
        "        if i % 12 == 0 and i > 0:\n",
        "            print(f\"  Still loading... ({i * 5}s elapsed)\")\n",
        "        time.sleep(5)\n",
        "else:\n",
        "    raise RuntimeError(\"vLLM timed out after 30 minutes. Check: !cat /content/vllm.log\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Start DocGemma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import subprocess, time, urllib.request, os\n",
        "\n",
        "# Kill any existing backend process\n",
        "subprocess.run([\"pkill\", \"-f\", \"docgemma.api.main\"], capture_output=True)\n",
        "time.sleep(2)\n",
        "\n",
        "env = os.environ.copy()\n",
        "env.update({\n",
        "    \"DOCGEMMA_ENDPOINT\": f\"http://localhost:{VLLM_PORT}\",\n",
        "    \"DOCGEMMA_API_KEY\": \"token\",\n",
        "    \"DOCGEMMA_MODEL\": MODEL,\n",
        "    \"PATH\": f\"{os.path.expanduser('~')}/.local/bin:{env.get('PATH', '')}\",\n",
        "})\n",
        "\n",
        "app_log = open(\"/content/docgemma.log\", \"w\")\n",
        "app_proc = subprocess.Popen(\n",
        "    [\"uv\", \"run\", \"uvicorn\", \"docgemma.api.main:app\",\n",
        "     \"--host\", \"0.0.0.0\", \"--port\", str(APP_PORT)],\n",
        "    cwd=f\"{WORKDIR}/docgemma-connect\",\n",
        "    stdout=app_log, stderr=subprocess.STDOUT,\n",
        "    env=env,\n",
        ")\n",
        "print(f\"DocGemma starting (PID: {app_proc.pid})...\")\n",
        "\n",
        "# Wait for health\n",
        "for i in range(30):\n",
        "    try:\n",
        "        urllib.request.urlopen(f\"http://localhost:{APP_PORT}/api/health\", timeout=2)\n",
        "        print(f\"DocGemma is ready on port {APP_PORT}!\")\n",
        "        break\n",
        "    except Exception:\n",
        "        if app_proc.poll() is not None:\n",
        "            print(\"DocGemma failed to start. Log:\")\n",
        "            !tail -20 /content/docgemma.log\n",
        "            raise RuntimeError(\"DocGemma failed to start.\")\n",
        "        time.sleep(2)\n",
        "else:\n",
        "    raise RuntimeError(\"DocGemma timed out. Check: !cat /content/docgemma.log\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Create Public URL\n",
        "\n",
        "Creates a Cloudflare tunnel to expose DocGemma via a public URL. No signup required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import subprocess, time, re\n",
        "\n",
        "# Kill any existing tunnel\n",
        "subprocess.run([\"pkill\", \"-f\", \"cloudflared\"], capture_output=True)\n",
        "time.sleep(1)\n",
        "\n",
        "tunnel_log = open(\"/content/tunnel.log\", \"w\")\n",
        "tunnel_proc = subprocess.Popen(\n",
        "    [\"cloudflared\", \"tunnel\", \"--url\", f\"http://localhost:{APP_PORT}\",\n",
        "     \"--no-autoupdate\"],\n",
        "    stdout=tunnel_log, stderr=subprocess.STDOUT,\n",
        ")\n",
        "\n",
        "# Wait for tunnel URL to appear in logs\n",
        "public_url = None\n",
        "for i in range(30):\n",
        "    time.sleep(2)\n",
        "    try:\n",
        "        with open(\"/content/tunnel.log\", \"r\") as f:\n",
        "            log_content = f.read()\n",
        "        match = re.search(r\"(https://[a-z0-9-]+\\.trycloudflare\\.com)\", log_content)\n",
        "        if match:\n",
        "            public_url = match.group(1)\n",
        "            break\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "if public_url:\n",
        "    print(\"\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"  DocGemma is live at:\")\n",
        "    print(f\"  {public_url}\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\")\n",
        "    print(f\"Model: {MODEL}\")\n",
        "    print(f\"GPU:   {gpu_name}\")\n",
        "    print(\"\")\n",
        "    print(\"The URL stays active as long as this notebook is running.\")\n",
        "    print(\"To stop: Runtime > Disconnect and delete runtime\")\n",
        "else:\n",
        "    print(\"Failed to create tunnel. Log:\")\n",
        "    !cat /content/tunnel.log\n",
        "    print(f\"\\nYou can still access DocGemma locally at: http://localhost:{APP_PORT}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Debugging\n",
        "\n",
        "Run these cells if something goes wrong."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check vLLM logs\n",
        "!tail -30 /content/vllm.log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check DocGemma logs\n",
        "!tail -30 /content/docgemma.log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check tunnel logs\n",
        "!tail -30 /content/tunnel.log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check all processes are running\n",
        "!ps aux | grep -E 'vllm|docgemma|cloudflared' | grep -v grep"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test health endpoints\n",
        "import urllib.request, json\n",
        "try:\n",
        "    resp = urllib.request.urlopen(f\"http://localhost:{VLLM_PORT}/health\")\n",
        "    print(f\"vLLM:     OK ({resp.status})\")\n",
        "except Exception as e:\n",
        "    print(f\"vLLM:     FAILED ({e})\")\n",
        "\n",
        "try:\n",
        "    resp = urllib.request.urlopen(f\"http://localhost:{APP_PORT}/api/health\")\n",
        "    data = json.loads(resp.read())\n",
        "    print(f\"DocGemma: OK ({data})\")\n",
        "except Exception as e:\n",
        "    print(f\"DocGemma: FAILED ({e})\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}