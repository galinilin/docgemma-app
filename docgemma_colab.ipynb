{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DocGemma on Google Colab\n",
    "\n",
    "Agentic medical AI assistant powered by MedGemma, with autonomous tool calling for clinical decision support.\n",
    "\n",
    "This notebook deploys the full DocGemma stack (vLLM + backend + frontend) and exposes it via a public URL.\n",
    "\n",
    "**Requirements:**\n",
    "- **A100 GPU** runtime (Runtime > Change runtime type > A100)\n",
    "- **High RAM** — required for MedGemma 27B, optional for 1.5 4B\n",
    "- [HuggingFace token](https://huggingface.co/settings/tokens) with access to [MedGemma](https://huggingface.co/google/medgemma-27b-it)\n",
    "\n",
    "**Runtime setup:**\n",
    "1. Go to **Runtime > Change runtime type**\n",
    "2. Set **Hardware accelerator** to **A100 GPU**\n",
    "3. Enable **High RAM** (required for 27B)\n",
    "4. Click **Save**\n",
    "\n",
    "| GPU | VRAM | High RAM | Supported Models |\n",
    "|-----|------|----------|------------------|\n",
    "| T4 | 16 GB | — | Not supported — no bfloat16 or Flash Attention 2 (compute capability 7.5) |\n",
    "| A100 (40 GB) | 40 GB | optional | MedGemma 1.5 4B |\n",
    "| A100 (80 GB) | 80 GB | required | MedGemma 27B, MedGemma 1.5 4B |\n",
    "\n",
    "**Repos:** [docgemma-app](https://github.com/galinilin/docgemma-app) | [docgemma-connect](https://github.com/galinilin/docgemma-connect) | [docgemma-frontend](https://github.com/galinilin/docgemma-frontend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Select your model and enter your HuggingFace token below, then run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title DocGemma Configuration { run: \"auto\" }\n",
    "#@markdown Select model and enter your HuggingFace token.\n",
    "\n",
    "MODEL = \"google/medgemma-27b-it\" #@param [\"google/medgemma-27b-it\", \"google/medgemma-1.5-4b-it\"] {type:\"string\"}\n",
    "HF_TOKEN = \"\" #@param {type:\"string\"}\n",
    "\n",
    "import subprocess, os, re, time\n",
    "\n",
    "# --- Validate HF token ---\n",
    "if not HF_TOKEN:\n",
    "    raise ValueError(\"HuggingFace token is required. Get one at https://huggingface.co/settings/tokens\")\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = HF_TOKEN\n",
    "\n",
    "# --- Check GPU ---\n",
    "result = subprocess.run([\"nvidia-smi\", \"--query-gpu=name,memory.total,compute_cap\", \"--format=csv,noheader,nounits\"],\n",
    "                        capture_output=True, text=True)\n",
    "if result.returncode != 0:\n",
    "    raise RuntimeError(\"No GPU detected. Go to Runtime > Change runtime type > A100.\")\n",
    "\n",
    "parts = result.stdout.strip().split(\", \")\n",
    "gpu_name, gpu_mem = parts[0], int(parts[1])\n",
    "compute_cap = parts[2] if len(parts) > 2 else \"unknown\"\n",
    "\n",
    "# --- Validate GPU compatibility ---\n",
    "if \"T4\" in gpu_name or (compute_cap != \"unknown\" and float(compute_cap) < 8.0):\n",
    "    raise RuntimeError(\n",
    "        f\"{gpu_name} (compute capability {compute_cap}) is not supported.\\n\"\n",
    "        f\"MedGemma requires bfloat16 and Flash Attention 2 (compute capability >= 8.0).\\n\"\n",
    "        f\"Go to Runtime > Change runtime type > select A100.\"\n",
    "    )\n",
    "\n",
    "if \"27b\" in MODEL and gpu_mem < 48000:\n",
    "    raise RuntimeError(\n",
    "        f\"MedGemma 27B requires ~48GB VRAM but {gpu_name} has {gpu_mem}MB.\\n\"\n",
    "        f\"Either switch to 'google/medgemma-1.5-4b-it' or use an A100 (80GB).\"\n",
    "    )\n",
    "\n",
    "# --- Check system RAM (High RAM required for 27B) ---\n",
    "import psutil\n",
    "ram_gb = psutil.virtual_memory().total / (1024 ** 3)\n",
    "if \"27b\" in MODEL and ram_gb < 30:\n",
    "    raise RuntimeError(\n",
    "        f\"System RAM is {ram_gb:.0f} GB. MedGemma 27B requires High RAM enabled.\\n\"\n",
    "        f\"Go to Runtime > Change runtime type > enable High RAM, then click Save.\"\n",
    "    )\n",
    "\n",
    "# --- Preflight: verify Cloudflare tunnel works ---\n",
    "print(\"Checking Cloudflare tunnel availability...\")\n",
    "\n",
    "# Install cloudflared if not present\n",
    "if subprocess.run([\"which\", \"cloudflared\"], capture_output=True).returncode != 0:\n",
    "    subprocess.run([\n",
    "        \"bash\", \"-c\",\n",
    "        \"curl -fsSL https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -o /usr/local/bin/cloudflared && chmod +x /usr/local/bin/cloudflared\"\n",
    "    ], check=True, capture_output=True)\n",
    "\n",
    "# Start a test tunnel on a dummy port\n",
    "test_log = open(\"/tmp/tunnel_test.log\", \"w\")\n",
    "test_proc = subprocess.Popen(\n",
    "    [\"cloudflared\", \"tunnel\", \"--url\", \"http://localhost:19999\", \"--no-autoupdate\"],\n",
    "    stdout=test_log, stderr=subprocess.STDOUT,\n",
    ")\n",
    "\n",
    "tunnel_ok = False\n",
    "for _ in range(15):\n",
    "    time.sleep(2)\n",
    "    with open(\"/tmp/tunnel_test.log\", \"r\") as f:\n",
    "        if re.search(r\"https://[a-z0-9-]+\\.trycloudflare\\.com\", f.read()):\n",
    "            tunnel_ok = True\n",
    "            break\n",
    "\n",
    "test_proc.terminate()\n",
    "test_proc.wait()\n",
    "\n",
    "if not tunnel_ok:\n",
    "    with open(\"/tmp/tunnel_test.log\", \"r\") as f:\n",
    "        print(f.read())\n",
    "    raise RuntimeError(\n",
    "        \"Cloudflare tunnel failed to start. The app won't be accessible without a tunnel.\\n\"\n",
    "        \"This is usually a temporary Cloudflare issue — try again in a few minutes.\"\n",
    "    )\n",
    "\n",
    "print(\"Tunnel check passed.\")\n",
    "\n",
    "# --- Ports ---\n",
    "VLLM_PORT = 8000\n",
    "APP_PORT = 8081\n",
    "WORKDIR = \"/content/docgemma\"\n",
    "os.makedirs(WORKDIR, exist_ok=True)\n",
    "\n",
    "print(f\"GPU:              {gpu_name} ({gpu_mem} MB VRAM)\")\n",
    "print(f\"Compute cap:      {compute_cap}\")\n",
    "print(f\"System RAM:       {ram_gb:.0f} GB\")\n",
    "print(f\"Model:            {MODEL}\")\n",
    "print(f\"Token:            {HF_TOKEN[:8]}...\")\n",
    "print(f\"\\nAll preflight checks passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies\n",
    "\n",
    "Installs Node.js, UV (Python package manager), and vLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "\n",
    "echo \"=== Installing Node.js ===\"\n",
    "if ! command -v node &>/dev/null || [ \"$(node --version | grep -oP '(?<=v)\\d+')\" -lt 18 ]; then\n",
    "    curl -fsSL https://deb.nodesource.com/setup_20.x | bash - > /dev/null 2>&1\n",
    "    apt-get install -y -qq nodejs > /dev/null 2>&1\n",
    "fi\n",
    "echo \"Node.js $(node --version)\"\n",
    "\n",
    "echo \"=== Installing UV ===\"\n",
    "if ! command -v uv &>/dev/null; then\n",
    "    curl -LsSf https://astral.sh/uv/install.sh | sh 2>/dev/null\n",
    "fi\n",
    "export PATH=\"$HOME/.local/bin:$PATH\"\n",
    "echo \"UV $(uv --version)\"\n",
    "\n",
    "echo \"=== Done ===\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install vLLM + HuggingFace CLI (takes a few minutes)\n",
    "!pip install -q vllm huggingface_hub\n",
    "!huggingface-cli login --token $HF_TOKEN 2>/dev/null\n",
    "print(\"vLLM + HuggingFace ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clone & Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "export PATH=\"$HOME/.local/bin:$PATH\"\n",
    "WORKDIR=\"/content/docgemma\"\n",
    "\n",
    "echo \"=== Cloning repositories ===\"\n",
    "if [ ! -d \"$WORKDIR/docgemma-connect\" ]; then\n",
    "    git clone --depth 1 https://github.com/galinilin/docgemma-connect.git \"$WORKDIR/docgemma-connect\"\n",
    "else\n",
    "    echo \"docgemma-connect already cloned\"\n",
    "fi\n",
    "\n",
    "if [ ! -d \"$WORKDIR/docgemma-frontend\" ]; then\n",
    "    git clone --depth 1 https://github.com/galinilin/docgemma-frontend.git \"$WORKDIR/docgemma-frontend\"\n",
    "else\n",
    "    echo \"docgemma-frontend already cloned\"\n",
    "fi\n",
    "\n",
    "echo \"=== Installing backend dependencies ===\"\n",
    "cd \"$WORKDIR/docgemma-connect\"\n",
    "uv sync --frozen --no-dev\n",
    "\n",
    "echo \"=== Building frontend ===\"\n",
    "cd \"$WORKDIR/docgemma-frontend\"\n",
    "npm install --silent 2>/dev/null\n",
    "VITE_API_URL=/api npm run build\n",
    "\n",
    "echo \"=== Copying frontend into backend ===\"\n",
    "mkdir -p \"$WORKDIR/docgemma-connect/static\"\n",
    "cp -r \"$WORKDIR/docgemma-frontend/dist/\"* \"$WORKDIR/docgemma-connect/static/\"\n",
    "\n",
    "echo \"=== Build complete ===\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Start vLLM\n",
    "\n",
    "Starts the vLLM inference server in the background and waits for it to be ready. First run downloads model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, time, urllib.request\n",
    "\n",
    "# Kill any existing vLLM process\n",
    "subprocess.run([\"pkill\", \"-f\", \"vllm.entrypoints\"], capture_output=True)\n",
    "time.sleep(2)\n",
    "\n",
    "vllm_cmd = [\n",
    "    \"vllm\", \"serve\", MODEL,\n",
    "    \"--max-model-len\", \"8192\",\n",
    "    \"--gpu-memory-utilization\", \"0.90\",\n",
    "    \"--host\", \"0.0.0.0\",\n",
    "    \"--port\", str(VLLM_PORT),\n",
    "]\n",
    "\n",
    "vllm_log = open(\"/content/vllm.log\", \"w\")\n",
    "vllm_proc = subprocess.Popen(vllm_cmd, stdout=vllm_log, stderr=subprocess.STDOUT)\n",
    "print(f\"vLLM starting (PID: {vllm_proc.pid})...\")\n",
    "print(f\"Model: {MODEL}\")\n",
    "print(\"Waiting for model to load (check /content/vllm.log for progress)...\")\n",
    "\n",
    "# Wait for health endpoint\n",
    "for i in range(360):  # 30 minutes max\n",
    "    try:\n",
    "        urllib.request.urlopen(f\"http://localhost:{VLLM_PORT}/health\", timeout=2)\n",
    "        print(f\"\\nvLLM is ready! (took ~{i * 5}s)\")\n",
    "        break\n",
    "    except Exception:\n",
    "        if vllm_proc.poll() is not None:\n",
    "            print(\"\\nvLLM process died. Last 30 lines of log:\")\n",
    "            !tail -30 /content/vllm.log\n",
    "            raise RuntimeError(\"vLLM failed to start.\")\n",
    "        if i % 12 == 0 and i > 0:\n",
    "            print(f\"  Still loading... ({i * 5}s elapsed)\")\n",
    "        time.sleep(5)\n",
    "else:\n",
    "    raise RuntimeError(\"vLLM timed out after 30 minutes. Check: !cat /content/vllm.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Start DocGemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, time, urllib.request, os\n",
    "\n",
    "# Kill any existing backend process\n",
    "subprocess.run([\"pkill\", \"-f\", \"docgemma.api.main\"], capture_output=True)\n",
    "time.sleep(2)\n",
    "\n",
    "env = os.environ.copy()\n",
    "env.update({\n",
    "    \"DOCGEMMA_ENDPOINT\": f\"http://localhost:{VLLM_PORT}\",\n",
    "    \"DOCGEMMA_API_KEY\": \"token\",\n",
    "    \"DOCGEMMA_MODEL\": MODEL,\n",
    "    \"PATH\": f\"{os.path.expanduser('~')}/.local/bin:{env.get('PATH', '')}\",\n",
    "})\n",
    "\n",
    "app_log = open(\"/content/docgemma.log\", \"w\")\n",
    "app_proc = subprocess.Popen(\n",
    "    [\"uv\", \"run\", \"uvicorn\", \"docgemma.api.main:app\",\n",
    "     \"--host\", \"0.0.0.0\", \"--port\", str(APP_PORT)],\n",
    "    cwd=f\"{WORKDIR}/docgemma-connect\",\n",
    "    stdout=app_log, stderr=subprocess.STDOUT,\n",
    "    env=env,\n",
    ")\n",
    "print(f\"DocGemma starting (PID: {app_proc.pid})...\")\n",
    "\n",
    "# Wait for health\n",
    "for i in range(30):\n",
    "    try:\n",
    "        urllib.request.urlopen(f\"http://localhost:{APP_PORT}/api/health\", timeout=2)\n",
    "        print(f\"DocGemma is ready on port {APP_PORT}!\")\n",
    "        break\n",
    "    except Exception:\n",
    "        if app_proc.poll() is not None:\n",
    "            print(\"DocGemma failed to start. Log:\")\n",
    "            !tail -20 /content/docgemma.log\n",
    "            raise RuntimeError(\"DocGemma failed to start.\")\n",
    "        time.sleep(2)\n",
    "else:\n",
    "    raise RuntimeError(\"DocGemma timed out. Check: !cat /content/docgemma.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Public URL\n",
    "\n",
    "Creates a Cloudflare tunnel to expose DocGemma via a public URL. No signup required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, time, re\n",
    "\n",
    "# Kill any existing tunnel\n",
    "subprocess.run([\"pkill\", \"-f\", \"cloudflared\"], capture_output=True)\n",
    "time.sleep(1)\n",
    "\n",
    "tunnel_log = open(\"/content/tunnel.log\", \"w\")\n",
    "tunnel_proc = subprocess.Popen(\n",
    "    [\"cloudflared\", \"tunnel\", \"--url\", f\"http://localhost:{APP_PORT}\",\n",
    "     \"--no-autoupdate\"],\n",
    "    stdout=tunnel_log, stderr=subprocess.STDOUT,\n",
    ")\n",
    "\n",
    "# Wait for tunnel URL to appear in logs\n",
    "public_url = None\n",
    "for i in range(30):\n",
    "    time.sleep(2)\n",
    "    try:\n",
    "        with open(\"/content/tunnel.log\", \"r\") as f:\n",
    "            log_content = f.read()\n",
    "        match = re.search(r\"(https://[a-z0-9-]+\\.trycloudflare\\.com)\", log_content)\n",
    "        if match:\n",
    "            public_url = match.group(1)\n",
    "            break\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if public_url:\n",
    "    print(\"\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  DocGemma is live at:\")\n",
    "    print(f\"  {public_url}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\")\n",
    "    print(f\"Model: {MODEL}\")\n",
    "    print(f\"GPU:   {gpu_name}\")\n",
    "    print(\"\")\n",
    "    print(\"The URL stays active as long as this notebook is running.\")\n",
    "    print(\"To stop: Runtime > Disconnect and delete runtime\")\n",
    "else:\n",
    "    print(\"Failed to create tunnel. Log:\")\n",
    "    !cat /content/tunnel.log\n",
    "    print(f\"\\nYou can still access DocGemma locally at: http://localhost:{APP_PORT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging\n",
    "\n",
    "Run these cells if something goes wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check vLLM logs\n",
    "!tail -30 /content/vllm.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check DocGemma logs\n",
    "!tail -30 /content/docgemma.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check tunnel logs\n",
    "!tail -30 /content/tunnel.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all processes are running\n",
    "!ps aux | grep -E 'vllm|docgemma|cloudflared' | grep -v grep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test health endpoints\n",
    "import urllib.request, json\n",
    "try:\n",
    "    resp = urllib.request.urlopen(f\"http://localhost:{VLLM_PORT}/health\")\n",
    "    print(f\"vLLM:     OK ({resp.status})\")\n",
    "except Exception as e:\n",
    "    print(f\"vLLM:     FAILED ({e})\")\n",
    "\n",
    "try:\n",
    "    resp = urllib.request.urlopen(f\"http://localhost:{APP_PORT}/api/health\")\n",
    "    data = json.loads(resp.read())\n",
    "    print(f\"DocGemma: OK ({data})\")\n",
    "except Exception as e:\n",
    "    print(f\"DocGemma: FAILED ({e})\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
